{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding WordEmbedding \n",
    "\n",
    "* Paper : Distributed Representations of Words and Phrases and their Compositionality\n",
    "\n",
    "* https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Type of Architectures:\n",
    "    \n",
    "* CBOW : It uses n words before and after target word $w_t$  to predict it.\n",
    "* Skip Gram: It uses the centre word to predict the surrounding word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "* CBOW Objective Function \n",
    "\n",
    "$J_{\\theta}= \\frac{1}{T} \\sum_{t=1}^T log p(w_{t}|w_{t-n}....,w_{t-n},w_{t+1}....,w_{t+n})$\n",
    "\n",
    "* Skip Gram Objective Function\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![CBOW Architecture](word2vec_diagrams.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "* Subsampling of frequent words during training increases the speed of training and also increases accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison between CBOW and Skip-Gram\n",
    "\n",
    "* Skip-gram: works well with small amount of the training data, represents well even rare words or phrases.\n",
    "\n",
    "* CBOW: Several times faster to train than the skip-gram, slightly better accuracy for the frequent words\n",
    "\n",
    "* This can get even a bit more complicated if you consider that there are two different ways how to train the models: the normalized hierarchical softmax, and the un-normalized negative sampling. Both work quite differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Will be using skip-gram model for the representation of word embedding:\n",
    "\n",
    "Why ?\n",
    "\n",
    "* Training Data is less and Skip-Gram behaves better with less training data as there \n",
    "can be different combinations\n",
    "\n",
    "* The paper below says that skip-gram model gives better results for semantic similarity.\n",
    "\n",
    "* There is a tradeoff with training time for skip gram.\n",
    "\n",
    "* Efficient Estimation of Word Representations in Vector Space \n",
    "  http://arxiv.org/pdf/1301.3781.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
